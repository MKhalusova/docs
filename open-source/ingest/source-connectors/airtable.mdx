---
title: Airtable 
description: Connect Airtable to your preprocessing pipeline, and batch process all your documents using `unstructured-ingest` to store structured outputs locally on your filesystem.
---

First you’ll need to install the Airtable dependencies as shown here.

    ```bash Shell
    pip install "unstructured[airtable]"
    ```
    


## Run Locally

<CodeGroup>
    ```bash Shell
    #!/usr/bin/env bash
    
    unstructured-ingest \
      airtable \
      --metadata-exclude filename,file_directory,metadata.data_source.date_processed \
      --personal-access-token "$AIRTABLE_PERSONAL_ACCESS_TOKEN" \
      --output-dir airtable-ingest-output \
      --num-processes 2 \
      --reprocess
    
    ```

        ```python Python
    import os
    
    from unstructured.ingest.connector.airtable import AirtableAccessConfig, SimpleAirtableConfig
    from unstructured.ingest.interfaces import (
        PartitionConfig,
        ProcessorConfig,
        ReadConfig,
    )
    from unstructured.ingest.runner import AirtableRunner
    
    if __name__ == "__main__":
        runner = AirtableRunner(
            processor_config=ProcessorConfig(
                verbose=True,
                output_dir="airtable-ingest-output",
                num_processes=2,
            ),
            read_config=ReadConfig(),
            partition_config=PartitionConfig(),
            connector_config=SimpleAirtableConfig(
                access_config=AirtableAccessConfig(
                    personal_access_token=os.getenv("AIRTABLE_PERSONAL_ACCESS_TOKEN")
                ),
            ),
        )
        runner.run()
    ```
    
</CodeGroup>


## Run via the API

You can also use upstream connectors with the `unstructured` API. For this you’ll need to use the `--partition-by-api` flag and pass in your API key with `--api-key`.

<CodeGroup>
    ```bash Shell
    #!/usr/bin/env bash
    
    unstructured-ingest \
      airtable \
      --metadata-exclude filename,file_directory,metadata.data_source.date_processed \
      --personal-access-token "$AIRTABLE_PERSONAL_ACCESS_TOKEN" \
      --output-dir airtable-ingest-output \
      --num-processes 2 \
      --reprocess \
      --partition-by-api \
      --api-key "<UNSTRUCTURED-API-KEY>"
    
    ```
    
        ```python Python
        import os
        
        from unstructured.ingest.connector.airtable import AirtableAccessConfig, SimpleAirtableConfig
        from unstructured.ingest.interfaces import (
            PartitionConfig,
            ProcessorConfig,
            ReadConfig,
        )
        from unstructured.ingest.runner import AirtableRunner
        
        if __name__ == "__main__":
            runner = AirtableRunner(
                processor_config=ProcessorConfig(
                    verbose=True,
                    output_dir="airtable-ingest-output",
                    num_processes=2,
                ),
                read_config=ReadConfig(),
                partition_config=PartitionConfig(
                    partition_by_api=True,
                    api_key=os.getenv("UNSTRUCTURED_API_KEY"),
                ),
                connector_config=SimpleAirtableConfig(
                    access_config=AirtableAccessConfig(
                        personal_access_token=os.getenv("AIRTABLE_PERSONAL_ACCESS_TOKEN")
                    ),
                ),
            )
            runner.run()
        ```
</CodeGroup>


Additionally, you will need to pass the `--partition-endpoint` if you’re running the API locally. You can find more information about the `unstructured` API [here](https://github.com/Unstructured-IO/unstructured-api).

For a full list of the options the CLI accepts check `unstructured-ingest airtable --help`.

NOTE: Keep in mind that you will need to have all the appropriate extras and dependencies for the file types of the documents contained in your data storage platform if you’re running this locally. You can find more information about this in the [installation guide](/installation/overview).